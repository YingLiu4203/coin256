# 不对称的防御能力

本书开头说过普通人只要抛 265 次硬币，就能为自己的信息与财产装上一把几乎不可破解的锁。这一听上去像传奇的说法，其实只是把“自由”落在“随机性”与“密钥”上，265 次独立抛掷汇成一串 0/1 的随机序列，足以构成一把任何现有计算机都无法穷举的私钥。握住这把钥匙，你就能为通信上锁、为文件落章、为资产签名；没有你的钥匙，任何平台、任何人都无法代替你动用你的身份与财产。自由不再仰赖善意或审批，而是化成一条在你掌心里的数学事实。本章要做的，就是把这条“从硬币到自由”的路径拆开讲清楚。先从随机性出发，说明为什么一串好随机数是所有安全的地基。再看对称加密如何高效地把聊天、相册、备份锁住，非对称加密如何把“身份”和“秘密”分离——公钥像名片可以公开，私钥像印章只在你手里，由此衍生出数字签名与无密码认证。然后引入哈希这枚“指纹与封条”，让“是否被改动”一眼可验。最后抵达零知识证明：在不泄露生日、住址、余额或私钥的前提下，依然能向世界证明“我已成年”和“这笔转账有效”。这些积木拼在一起，让个人拥有强大的不对称防御关键能力。

## 1 技术分权

技术的真正意义，不仅在于提高效率，更在于重组权力。在人类历史上，技术的出现往往伴随着权力的转移。有些技术让权力集中，有些则让权力分散。而每一次分权，都会带来个人自由的一次跃升。

第一次真正释放个体力量的技术，是十五世纪的印刷术。那时的知识稀缺而昂贵，文字掌握在神职人员和贵族手中。人们若想接触思想，就必须依附于宗教或权威。当古腾堡的印刷机开始运转，世界悄悄地改变了。书籍可以被大量复制，思想可以被广泛传播，个人第一次拥有了绕过权威去说话的机会。印刷术让知识走出了修道院，也让人走出了沉默。它不仅改变了文化传播的方式，更改变了社会结构，人开始自己阅读《圣经》，自己判断真理。宗教改革、启蒙运动、科学革命，都是这场分权运动的延续。印刷术把“表达的权力”从少数人手中交还给了每一个愿意思考的人。

五百年后，人类又一次陷入权力的集中。这一次的中心不在教会，而在信息服务器。人们的通信、财产、身份都被数字化，而控制这些数据的，不是牧师，而是平台和算法。人们依旧可以说话，但被算法决定能被谁听见；依旧可以拥有财产，但财产只是数据库中的数字；依旧有身份，却依附在系统发放的账号之下。

在这种背景下，加密技术的出现有了新的意义。它不只是计算机科学的进步，而是一种自由的再分配。加密让人们重新拥有了信任自己与他人的能力，人们可以用自己的密钥保护通信，用自己的签名证明身份，也可以存储和转移价值，而不依赖任何机构。加密学孕育了一种新的启蒙，它教人不再把信任寄托于权威，而是依赖于坚实的数学理论与公开的加密算法，让个人重新成为自己世界的中心。而这正是信息时代新的分权革命的开始，它不以暴力为手段，不以政治为旗帜，而以代码为语言，以数学为契约，在安静而深远的层面上，重新定义了和赋能了自由这个古老的词。

## 2 密码学素养

在所有人都关心都会涉及的信息安全领域，密码学就像识字：你不必成为语法学家，才能读写；但若连字母都不认，许多权利只能“听别人念给你听”。使用智能手机不需要成为电子或软件工程师，同样地，使用加密来创建数字身份、保护隐私也不需要数学学位。然而，懂得一些最基本的密码学概念——例如“私钥像印章且不可外借、公钥是可公开的名片、数字签名是可验证的承诺、哈希是内容的指纹、随机性与备份是安全的底座”——会带来三个层面的收益：第一，你能把信任从“相信”升级为“可验证”，知道系统到底在向你承诺什么以及背后的机制；第二，你能在关键节点做对选择：该不该分享一段信息、为何要备份密钥；第三，你能识别并拒绝社会工程与伪装的“好意”，明白“不是你的密钥，就不是你的资产”。基础密码学素养并非技术崇拜，而是一种自由的素养。它让个体知道自由的边界在哪里、安全的路径在哪里、哪些安全是系统替你完成的、哪些决定只能由你来下。正如交通规则让驾驶变得安全顺畅，基本的密码学常识让“把复杂交给系统，把确定还给人”真正落地。

### 2.1 二个密码学基本常识

密码学有二个有点违背一般人认知的常识：一是不要自己开发私有的加密系统，二是安全性只来自密钥的保密，而不是算法的神秘。安全不是“点子巧”，而是要经多年公开挑战与攻击仍站得住。未经公众大量审核的私有的加密系统一定有很多漏洞。密码学历史上有很多类似下面 DVD 播放机加密系统的例子。

1999 年是 DVD 播放机的高光时刻。为防止未经授权复制和分发受版权保护的电影内容，那一代 DVD Video 采用的是行业联盟的加密方案 CSS（Content Scramble System）：算法不公开、播放器里预置了“设备密钥”，只有拿到授权的机器才能把影碟放出来。设计者的直觉是：“把算法藏起来，把密钥焊进机器里，安全就万无一失。” 很快，现实教育了所有人。一名 15 岁的挪威少年和一群黑客从某款软件播放器里抓出了设备密钥，又把 CSS 的“秘密算法”逆向了个底朝天，写出了著名的 DeCSS。从那天起，CSS 的“秘密”不再是秘密，任何人都能在普通电脑上读出 DVD 里的内容。你可以起诉、下架、发律师函，但信息一旦公开就无法回收——这就是“靠保密算法来保安全”的尴尬结局。

这件事正好印证了两条密码学里最重要的常识：

- 施奈尔定律（Schneier’s Law）：任何人都能设计出一个自己看不出破绽的加密系统；但这不等于它是安全的。换句话说，“我觉得挺牢靠”不等于“经得住全世界挑刺”。
- Kerckhoffs 原则：一个密码系统即使除了密钥之外的一切都公开，也应该是安全的。真正可靠的加密，安全性只来自密钥的保密，而不是算法的神秘。像 AES、ChaCha2 这类现代加密算法，完整细节都写在公开标准里，依然安全，因为没人知道你的密钥。信息论的发明者克劳德·香农则改成说：“敌人了解系统”，这样的说法则称为香农箴言。 它和传统上使用隐密的设计、实现、或其他等等来提供加密的隐晦式安全想法相对。

回看 CSS：它把希望押在“算法保密 + 设备里藏钥匙”。可只要播放就必须把密钥交到终端，而终端在攻击者手里，算法就会被逆向、密钥就会被抠出来。所以，在所有的密码系统的开放和使用中，务必谨记这两句话：别指望算法保密来撑安全，要使用经审计的公开算法与协议，让全世界先替你找洞。把全部“秘密”收敛到密钥，密钥可轮换、可撤销、可备份。

### 2.2 一些基本概念

密码学听上去像高等数学，其实也确实比较复杂和抽象。幸运的是作为用户其实要掌握的基本概念非常少，也不需要微积分或抽象代数。会加减乘除、有概率的直觉（像一百万次里也难猜中）、能读懂几句比喻，就足以把它用在自己的身份、通信与财产上，而且这种收益会在此后许多年持续“复利”。你真正需要懂的，就下面会给出详细解释的这几个观念：

- 随机性：密码学的地基，强随机数让最快的计算机也难以被猜中。
- 对称加密算法：加密和解密用同一把密钥。
- 非对称加密算法：加密和解密用不同的密钥（公钥和私钥）。
- 哈希算法：把任意内容压成一小串“指纹”，相同内容指纹必然相同、改一字指纹全变。
- 零知识证明：提供必要的证据的同时可以保护隐私。

## 3 随机性

随机性是指不可预测性，越是不可以预测的事物随机性就越强。随机数就是不可预测的数字序列。加密、解密、消息编号、口令存储、数字签名都很多地方都需要用到随机数。比如加密解密用到的密钥越随机，别人就越不可能“配出同样的钥匙”。信息安全的第一步通常是生成随机数，而随机数的不可预测性决定了信息安全的程度。人们为此大费周章也不足为奇了。

Cloudflare 是一家提供全球网络平台的公司，把内容分发（CDN）、安全防护与应用加速做成一体化的云服务，目标是让网站与互联网应用更快、更可靠、也更难被攻击。如果你有机会走进该公司旧金山总部的大厅，第一眼可能不是前台，而是一整面发着温暖光晕的熔岩灯墙。大小不一的蜡团在玻璃管里慢慢升腾、分裂、汇合，像一面流动的星图。对来访者来说它只是漂亮；对这家公司来说，它是“互联网的守门人”。天花板上方有一台镜头，持续拍摄这面墙，把每一帧里不可预测的细节——蜡团的形状、灯光的闪烁、甚至人群经过投下的阴影柔和起来提炼为“熵”（熵支不确定性），再喂给密码学安全随机数发生器，去增强网站连接和密钥生成的随机性。如果你恰好站在这面墙前，挥挥手、走过灯影，你的动作也会被相机“抓住”，成为那一瞬间随机性的组成部分。任何人都无法预言下一滴蜡会怎样拧、往哪里飘。正因为这种微观的不确定，互联网每天无数次握手的安全，得以多一层“来自现实的背书”。

![Lava light](./imgs/ch5_lava.jpg)
[Source: Wikipedia Lavarand](https://en.wikipedia.org/wiki/Lavarand)

普通人当然不太可能建这样一堵墙来获得有足够强度的随机性。其实有更简单的办法，虽然有点费时，但是随机性可以媲美熔岩墙。答案就来自足球比赛转播里常见的一幕：两支球队开赛前，要用抛硬币决定谁先开球。硬币在空中不停翻转，落地只会是正面或反面。这个小小的仪式有一种朴素的力量：在它落地之前，没人知道结果。这种不可预知，正是“随机性”。如果把“正面”记作 1、“反面”记作 0，一次公平的抛掷就给出一个二选一的答案。在信息的语言里，这份“是/否”的不确定量叫作 1 比特：像一盏小小的开关，只能开或关，却确实装下了一点“未知”。

![Coin Toss](./imgs/ch5_toin_toss.jpg)
[Source: Wikipedia Coin Flipping](https://en.wikipedia.org/wiki/Coin_flipping)

如果你不止抛一次，而是连续抛 256 次，把每次的结果依次写成由 0 和 1 的组成的长串。你得到的，不再是一盏开关，而是一整排 256 个小开关的组合，这就是 256 比特的随机数。它有多少种可能？答案是 2 的 256 次方，写成十进制约是 1 后面跟 77 个零。如果你觉得大得难以想象，你是对的，因为这个数字接近可观测宇宙里的原子总数。

这意味着什么？假如有台性能夸张到离谱的机器，每秒能试出 10¹⁸ 个候选密钥（万亿次中的万亿次），并且从宇宙大爆炸开始就不眠不休地算到今天（约 4×10¹⁷ 秒），它也只来得及试 10³⁵ 个组合。和上述可能性相比，连海边的一粒沙都算不上。换句话说，靠暴力穷举去猜中那串 265 次抛掷的结果，在物理上几乎不可能。因此，用这串位作为密钥或产生密钥的种子，等于给自己锻造了一把几乎不可能被复制的钥匙。

连续抛 256 次硬币除了有点费时间，看上去简单的难以置信。如果想加速，可以一家人同时抛不同的硬币把结果合并也可以，对于需要保护重要资产比如大量比特币的密钥，这种努力仍然值得。有人可能质疑抛硬币产生随机数看上去不够正规，这种担心也大可不必，因为这是互联网工程任务组 IETF（The Internet Engineering Task Force）推荐的随机数生成办法之一，参考 [IETF RFC 4086](https://datatracker.ietf.org/doc/html/rfc4086)。当然多数时候直接使用软件生成随机数就够用了。现代计算机操作系统自带的密码学安全随机数生成器从多个物理/系统事件取“熵”（键盘/网卡抖动、硬件噪声、CPU 指令等），混合后用经过分析的算法生成成高质量的随机数。

从这里开始，隐私与信任不再只是口头的承诺，而是可验证、可执行的技术事实；而那份由个人亲手掷出或计算机操作系统产生的随机性，正是个人在数字世界里主权的地基。

## 4 对称加密算法

加密大致分两类：对称加密和非对称加密。对称加密用同一把密钥加密与解密。非对称加密使用一对密钥：公钥和私钥。二种加密方式有各自的特点与应用场景，现实协议比如网络浏览器或端到端聊天等通常采用二种方式都用的混合加密。这里先介绍有数千年历史的对称加密概念。

对称加密（symmetric encryption）是人类最古老也最直观的加密方式：同一把密钥既用来把信息“上锁”，也用来把它“开锁”。只要通信双方事先约好并保守这把共同的秘密，别人就看不懂内容。这种“共用一把钥匙”的思路，贯穿了几千年的保密史。

### 4.1 纸笔时代的对称加密

在纸笔时代，“对称加密”的主力就是替换（substitution）与置换（transposition）两大类手工方法。替换是把原文中的每个符号按一条规则换成另一个符号。最有名的例子是恺撒密码：把字母表整体向后挪位，比如每个字母都往后移 3 位，A→D、B→E、…、X→A、Y→B、Z→C，“HELLO”就变成“KHOOR”。更一般的做法是准备一张对照表（密钥），把 26 个字母打乱映射成 26 个新的符号或字母——这叫单表替换。为了对抗“字母频率分析”（e 出现最多等），后来又出现多表替换（如维吉尼亚密码）：不同位置的字母用不同的替换表，让同一个字母在不同位置可能被替成不同符号，从而掩盖频率特征。置换则不改动字母本身，而是打乱顺序。常见方法有栅栏（rail fence）/列移位（columnar）：先把原文按行或按列写入表格，再按约定的顺序读出，达到“洗牌”的效果。比如按 4 列排成矩阵后按列读出，原字不变，但顺序已被密钥（列顺序）打散。用这种对称加密的密钥就是用到的那些密码参数比如挪位数、对照表或列顺序的值。

实际使用中，人们常把两者组合：先替换再置换，既改变字母“长相”，又打乱“队形”，以同时增加“难猜度”和“难还原度”。这些方法的共同点是同一把密钥用于加密与解密（例如挪位数、对照表或列顺序），完全符合对称加密的定义。如果把替换表的数目增加到与明文一样长，就是每个位置都用不同的替换表，并且完全随机且只用一次，就成了理论上“完美保密”的一次性密码本 OTP（one-time pad）。但在实际传递与管理上很不方便，这也是它难以普及的原因。

### 4.2 计算机时代的对称加密

#### 4.2.1  混淆与扩散

计算机时代，人们对密码学有了更深的认识。信息论之父克劳德·香农提出一套好的加密系统要同时做到混淆与扩散。

混淆就是让密文与密钥/明文之间的关系尽量复杂、非线性（不成比例、不可叠加），因此难以破解密钥。攻击者即使观察大量密文也难以建立方程或找出可预测的模式。比如不同的位置用不同的替换表，同一明文字母在不同位置被替成不同字母，混淆更强。而且只要密钥改动一点点，输出的密文变化也是乱得没有规律，想通过规律发现去倒推密钥，几乎行不通。

扩散则是让明文的微小改动能在密文里大范围蔓延（雪崩效应），打散统计特征。比如明文仅仅翻 1 个比特，理想情况下密文约有一半比特都会翻转，同时把明文的统计特征（如字母频率）打散到整个密文，避免被抓特征。可以想象为明文的一点改动，就如同把一滴墨滴进清水并充分搅拌，很快整个杯子都变色，局部信息的变化不再停留在局部，而是扩散到全局。

#### 4.2.2 分组加密与流加密

计算机时代的对称加密方法有分组加密与流加密二种实现方式，都围绕混淆与扩散这两个目标，

分组加密把数据切成固定大小的“块”，如 128 位，256 位，512 位等的数据块，对每块反复执行多轮运算，每一轮都在混淆与扩散之间交替叠加，直到达到“雪崩”。单块加密只是开始，还要考虑多块数据如何串起来做出强混淆、强扩散的密文。直觉上，分组加密像把每一块数据丢进多层搅拌机（替换）与多层洗牌机（置换）反复处理，前面的数据块还影响后面的数据块，多轮之后产生的密文变得既乱又散，统计痕迹被抹平。

流加密不是在明文上反复“替换＋置换”，而是先生成一条不可预测的密钥流，再与明文逐位操作后。安全的关键在于通过非线性的转换以及多轮的混合让密钥流做到既混淆又扩散。直觉上，流加密像一台“强随机发电机”，生成的电流（密钥流）与明文相加抵消出密文。只要发电机内部搅拌足够非线性、扩散足够快，外部就看不出下一秒电流的规律。

### 4.3 对称加密的特点

对称加密有悠久的历史，思想简单，实现成熟。基于计算机的对称加密算法操作速度快、开行小、还有很多硬件加速。对称加密适合大数据量与实时场景，生成一把临时会话密钥即可高速通信，便于轮换与前向保密（密钥定期更新，旧内容解不开）。常见的应用包括数据静态加密（文件与数库加密、云备份与归档）与高吞吐通信（虚拟专用网 VPN、视频通话、流媒体、消息加密）。

但对称加密的安全性不仅取决于“有一把好钥匙”，还取决于这把钥匙能否经常轮换：频繁更换会话密钥能把泄露的影响局限在最小时间窗（前向保密）、减少长期密钥被重放或离线破解的风险，也便于在设备丢失、人员变更时迅速撤销。但难点随之而来，密钥分发是对称加密体系的短板。双方必须先在一个已受保护的通道内拿到同一把新钥匙，形成“先有安全通道还是先有密钥”的鸡生蛋问题。

随着互联网的普及，当参与方从两个人增长到很多个人或很多进程（群聊、多人协作、微服务间通信），每次轮换对称密钥这件事会迅速从“换一把钥匙”变成“换一座城市的钥匙”。两人只要共享一把会话密钥；N 个人若用两两共享的方式，理论上要管理 N(N−1)/2 把密钥，任何一方轮换都牵动一大片。10 个人之间的二二私密通信需要 45 把密钥，100 个人之间的二二私密通需要 4950 把密钥，是平方级别的快速增长，因而无法扩展。就算采用“全体共用一把群密钥”，每次成员加入/离开都必须重新生成并分发新群钥给其余所有人，离开者还要立刻失效。再叠加“一人多设备”的现实（手机+电脑+平板），密钥分发对象从“人”变成“设备集合”，复杂度仍是直线上升。

非对称加密算法的发明让互联网的安全通信与身份认证成为可能。

## 5 非对称加密算法

在密码学的漫长历史里，非对称加密是少数足以改写世界运行方式的发明之一。它把“你得相信我”变成“你可以验证我”，让陌生人也能在开放网络上安全协作。其奠基工作屡获计算机领域的最高荣誉：1976 年 Whitfield Diffie 与 Martin Hellman 提出公钥密码与数字签名的思想，因“为现代密码学作出关键贡献”获 2015 年图灵奖；1977 年 Ronald Linn Rivest、Adi Shamir 和 Leonard Max Adleman 提出的 RSA 算法（以三人姓的首字母命名）则让公钥密码走向实用，三人共同获得 2002 年图灵奖。今天浏览器地址栏里的“小锁”、手机里的无密码登录、操作系统的可信更新、端到端加密聊天、数字货币的钱包签名等几乎都立在非对称加密技术之上。在公钥密码诞生近半个世纪之后，我们已经拥有一整族更安全、更高效的新算法，甚至包括了面向未来的后量子方案。它们的数学基础各不相同，抗攻击模型与实现细节也更加讲究，但基本的思想与设计理念都和最初的版本相似。都是由一对“可公开—需保密”的密钥构成身份与加密能力，用难以逆向的数学问题建立单向安全门，再通过签名、加密或密钥交换，把陌生人之间的协作变成可验证的协议。对读者而言，尽管原理看似深奥，使用层面的概念其实很简单，而且在不同算法之间几乎不变。

### 5.1 公钥与私钥

密钥对（key pair）是非对称密码的核心：一把私钥（只属于你）与一把公钥（可以公开）。它们像是印章与名片的组合——名片给所有人，印章只在你手里。密钥对由算法（RSA、椭圆曲线、后量子等）从高质量随机数一次性生成。前文提到连续抛 256 次硬币就可以生成高质量随机数，不过现在多数智能手机或个人计算机都可以利用系统的一些随机物理特性生成高质量随机数。

产生密钥对的数学机制提供了二个保证：1）用其中一个密钥加密则只能用另一个解密；2）密钥对有单向关联特性：公钥是从私钥计算出来的，但从公钥反推私钥在计算上不可行。第一个可以用密钥对完成基本的加密解密功能。第二个则是上文所说的用难以逆向的数学问题建立的单向安全门。所以公钥是公开的，可以用作个人的身份标识或账户地址，但是从公开的公钥无法推导出私钥。私钥需要特别保密，不能泄露或丢失。拿到私钥的人就拥有了对应的身份或账户。如果私钥丢失，则失去对应的身份或账户。所以私钥需要特别的保密与保护措施。

由于有二个公钥与私钥二个密钥，因而有两种加密方向的用法：用私钥加密信息，用公钥解密；用公钥加密信息，用私钥解密。这二个用法都有一些很有意思的用途。

### 5.2 用私钥加密，用公钥解密

用个人的私钥加密发消息不是为了保密，任何人都可以用对应的公钥解密看到内容同时可以确认一个事实：这个消息是拥有对应私钥的那个人发布的，而且内容未被改动。这个操作同时完成了三件非常有用的功能：身份认证、消息完整性确认、不可否认。基于这一点，私钥加密、公钥解密的实际用途就是任何需要来源可验证、内容完整、不可抵赖的场景。

最常见的身份验证之一是网站访问。简化的流程是浏览器通过可靠方式，比如通过软件安装或身份证书，先获得 Web 网站的公钥。然后浏览器生成一个随机数发给 Web 服务器，服务器用私钥加密返回。随后浏览器用服务器的公钥解密验证了服务器的身份。

个人当然也可以这个用法随时生成主权个人身份。人先在本地独立生成一对密钥，私钥只在自己手里，用它对一段随机签名挑战。任何人拿到对应的公钥都能“解开”并验证这枚签名确实出自你、内容未被改动。由此，公钥就成为可公开、可迁移、可审计的身份凭证，而你对私钥的独占控制则构成主权的技术事实。无需注册于任何平台、无需把口令交给任何人，也不必请求第三方背书就能证明“我就是我”。更进一步，你可以为不同场景派生不同密钥、随时轮换或吊销（丢机仅吊销该设备密钥），并用熟人背书或证书把公钥与现实身份绑定。在所有这些过程中，身份的生成与使用都掌握在你自己手里——身份来自你能签名，而不是别人替你命名。在比特币的应用中，所有的交易、钱包转账都用私钥授权，另一方则用作为账户地址的公钥来验证交易的合法性。

所以用私钥加密，用公钥解密的用法解决了二个常见的信息安全问题：身份认证和数字签名。

### 5.3 用公钥加密，用私钥解密

如果别人用你的公钥加密发消息，则只有你能用私钥解密看到内容。实际多用混合加密，先随机构造一把对称密钥加密数据，再用公钥加密这把小密钥。 比如网站用新闻机构/组织的公钥加密来稿与附件，编辑端用私钥解密。分享文件先用随机数据密钥加密文件，上传加密文件到公共平台时用对方公钥加密所用的数据密钥，平台无法解密。只有收件人可以用私钥解开密钥，从而可以解密文件。在公开平台投票时，选票用计票方公钥加密，防止传输与存储阶段被窥探。

用公钥加密，用私钥解密解决了对称加密算法面临的密钥分发问题。每个人只需要拿到通信对方的公钥就可以创建高效通信使用的对称加密密钥。

### 5.4 非对称加密的特点与实际用法

非对称加密的最大优点，是把“先交换同一把钥匙”这道历史难题拿走：任何人都能用你的公钥给你加密、用你的公钥验证你的签名，你则用私钥解密与签名，由此同时解决机密性、身份认证与不可抵赖。它让开放网络里的陌生人可以安全协作，是浏览器小锁、系统更新签名、无密码登录、端到端通信与数字货币的共同基石。缺点也同样鲜明，这类运算慢、开销大，密文和密钥体积更“胖”，不适合大数据量持续加密。同时还要面对公钥绑定与证书管理的工程复杂度（公钥属于谁、如何吊销与轮换），以及实现不当带来的诸多风险。

因此，现实系统几乎一律采用混合加密，用非对称算法只做开场认证对方身份并协商出一把临时的会话对称密钥，接下来把所有大流量数据交给对称加密高速处理。这种搭配把双方优势叠加起来，既免去了密钥预分发，又获得高吞吐与低耗能，临时会话密钥还能频繁更换，带来前向保密与更小的泄露影响面。简单说就是公钥体系负责认识彼此、约定密钥，对称加密体系则负责快而稳地搬运数据。两者合用，才构成今天互联网安全的主流做法。

## 6 哈希算法（Hash Algorithm）

哈希算法做的是“打指纹”，可以把任意内容——一段信息、一笔交易、一封邮件、一张照片、一本书，甚至整块硬盘压成一串固定长度的数字指纹（常见如 256 位，对人眼通常写成 64 个十六进制字符）。它有两个直观特性。第一是可复现，同样的输入永远得到同样的指纹。第二是雪崩效应，只改动一个比特位，比如把“to be or not to be” 改成 “so be or not to be”, 第一个字符的 “t“ 和 ”s” 的通常编码只有一位区别，指纹看起来就像完全换了一串号码，几乎找不到相似性。如果用 SHA256 哈希算法，前者的输出是 `2e9d72a733ae5a6c1c1ebd6802a33dfc92854bfc3439878e5d77dac320995f54` 而后者是 `46048b5d57d246e2c3041f4b85bd009d76699b94c97e5c5465885ad18f629c90`, 这让哈希非常适合做有没有被改过的快速判定。后面还会看到，哈希算法的雪崩效应也被比特币用做其工作量证明机制。

更重要的是，哈希还是单向的。从指纹几乎不可能把原文“倒推”出来（这与加密不同，哈希不是“上锁再开锁”，而是“贴封条”）。直观类比是：你能把一封信放进封袋并压上蜡印（哈希），任何人看到蜡印形状都能确认“这只封条没被替换”，但没人能仅凭蜡印还原信里的内容。理论上哈希会存在碰撞（不同内容指纹相同），但对现代、抗碰撞的算法（如 SHA-256、SHA-512、BLAKE3），随机撞上的概率小到可以忽略。

哈希算法在信息安全里有很多用途。哈希在可以充当“验货员”。当你下载一份软件或系统镜像，官网会给出一串校验值。你在本地对下载的文件再算一遍哈希，两者一致，说明从服务器到你电脑这一路“一字未改”，若对不上，要么是网络传输损坏，要么是被人“掉包”。在云备份与相册中，哈希又变成“仓库管理员”。两张照片或两个文件只要哈希完全相同，系统就判定它们是同一份，便只存一份实体、其余做指针引用，既节省空间又加快同步。直观地看，哈希就像“指纹 + 封条”：前者用来区分是否同一件东西，后者用来证明封口未被撕开过。

在更深一层的基础设施里，哈希有着“承重梁”的作用。数字签名并不是直接对整份大文件“落章”，而是先把内容哈希成一段短指纹，再对这段指纹签名，验证时也只需复算一次哈希即可确认“来源可信且未被改动”，既快又安全。这种数字签名方式是信息安全的标准基础设施。此外，数字货币与审计日志利用哈希构建链式/树状结构，把海量记录的哈希层层叠加成链结构或默克尔树结构（这种树状结构以其发明者命名），任何一条记录被动过手脚，都会沿着层级牵动整条链或整棵树的根指纹，篡改无处遁形。至于口令存储，更不能保存明文：系统会为每位用户生成随机“盐”，把“口令 + 盐”一起做口令哈希，就算数据库泄露，攻击者也难以从哈希值倒推你的真实口令。考虑到互联网上每个用户有几十个口令，这可能是每个人都最常使用的哈希应用吧。

## 7 零知识证明 ZKP（Zero-Knowledge Proof）

其实生活中处处有零知识证明的需求，只不过在信息技术让其变成可能性之前，大家没有选择。零知识证明是指在不透露秘密本身的前提下，让别人“确信你确实知道这个秘密”或“某个断言是真的”。比如门卫只需确认你已成年，却不必知道你的具体生日，网站只需知道你拥有对应私钥，却不必得到你的私钥，交易所只需确认你的资产大于负债，却不必了解你的资产明细。零知识证明把“相信我”变成“你可以验证我”，但又不把隐私交出去。它满足三条性质：完备性（如果命题真，诚实证明者能说服验证者）、可靠性/健壮性（如果命题假，作假者几乎无法蒙混过关）、零知识性（验证者除了“命题为真”这一点，学不到任何额外信息）。零知识证明可以超越“匿名因此不可信”与“已验证且已KYC”这种二元对立，能够同时解决关于真实性和操纵性的担忧。可以在防范“外部老大哥”监控的同时避免了“内部老大哥”泄露隐私的担忧。零知识证明的最大优点是可以证明关于我们是谁以及我们拥有哪些权限的更细粒度的信息。

![zero knowledge proof](./imgs/ch5_zkp.png)
[source: Wikipedia Zero-knowledge proof](https://en.wikipedia.org/wiki/Zero-knowledge_proof)

如图是密码学知名的“阿里巴巴山洞”：洞呈环形，中间有一扇需要密语才能开启的魔法门，左右各有入口 A、B。证明者张三声称“我知道密语”，验证者李四想确认真假，但又不想知道密语本身。做法是：张三先随便选 A 或 B 进去，走到魔法门前等候；李四在洞外随机喊一句：“从 A（或 B）出来！”。如果张三真的会开门，他就能根据要求选择穿过魔法门或原路返回，从而按李四指定的出口现身；如果他不会开门，只能碰运气。除非她一开始就选对了李四稍后点名的那一侧，否则就被当场识破。把这个“挑战—回应”的过程重复很多轮，李四每次都随机点名 A 或 B，会开门的人每轮都能成功，不会的人成功率却以每轮减半的速度指数级下跌。这个过程体现了零知识证明的三性：完备性（Completeness）指若命题为真，张三确实会开门，他总能按要求出现，验证必然通过；可靠性/健壮性（Soundness）指若命题为假（不会开门），他每轮只有一半概率蒙对出口，重复多轮仍不被抓包的概率非常低，几乎不可能长期骗过李四；零知识性（Zero-Knowledge）指整个过程里，李四只知道“张三确实能在需要时穿门”，却没有学到密语是什么、门如何开启。零知识证明的妙处正在于此：把“我知道密语”变成“你可以确信我知道”，同时把密语完好地留在我手里。

工程上，早期证明常是交互式，类似上述的多轮挑战与回应。后来通过算法技巧，把交互式证明变成非交互式，把挑战转换为生成不可预测的哈希函数，于是证明者一次性产出一份短小、可公开检查的“证书”，验证者通过检查证书即可验证所要证明的真假，但又得不到任何额外的信息。ZKP 的应用场景已经非常丰富而接地气，这些年 ZKP 的技术发展非常快，验证的速度提高了很多倍，可以满足不少实际应用的需求。在数字货币领域，用零知识证明做隐私转账已经运行多年。在公开运行的网络上，隐私转账只有交易双方看到账户地址与金额，其他人可以验证交易的合法性但看不到细节。考虑到交易隐私权的普遍性，估计以后 ZKP 会像公钥一样成为普通人所知道技术术语。

## 8 个人自由的保障

把随机性、对称与非对称加密算法、哈希算法和 ZKP 这些概念放在一起看，可以发现这些技术元素其实是在帮个人把“我是谁、哪些是我的、哪些该公开”三件事变得可验证又不泄露隐私。随机性先打地基，让一串够强的随机数就像独一无二的钥匙坯，没人能猜中你的密钥。用它做成对称加密，聊天记录、相册备份、钱包数据都能高速上锁。再用非对称加密创建个人身份，同时把“身份”和“秘密”分开：公钥像名片对外，别人据此给你加密或验你的签名；私钥像印章只在你手里，你据此解密与签名。哈希算法则给一切内容按上“指纹”：下载的软件、发送的文件、发布的声明，都能一眼验真、发现篡改。最后，零知识证明 ZKP 让你在不暴露细节的前提下证明关键事实：不告诉生日也能证明已成年，不公开住址也能证明住在某地，不晒账本也能证明资产大于某个数额，不交出私钥也能证明确是我本人拥有。这些技术带来结果就是你的身份由密钥与证明来定义，而不是由平台来界定。你的隐私靠数学定律来守护，而不是依赖别人的承诺。信任从“请相信我”变成“你可以验证我”，个人在信息世界里第一次拥有了既能被认可、又能自己设边界的安全与自由。

这些技术还有一个不容忽视的独特价值，在于把个人的安全从“力气之争”变成“数学之争”，从而形成不对称防御：你只需在手机里妥善保管一把强随机生成的私钥，就能用对称/非对称加密把通信与资产“锁住”；而攻击者若想硬破，只能面对天文级别的搜索空间，个人的成本是几秒钟建钥与良好备份，攻击者的成本是不可实现的算力与时间。同样，端到端加密让窃听者即便握有全球路由，也只能得到无意义的密文。哈希与时间戳把“动过没动过”变成一眼可验的事实，篡改会立刻露馅。私钥签名把身份从平台授予改为可验证的自我声明，使得财产支配与身份认证不再仰赖中介。这种不对称还体现在最小披露上。基于数学理论的零知识证明让个人只证明需要证明的那一部分，既能满足合规或对手方的信任要求，又不给对方多一分可滥用的数据。结果是，个人用极低的边际成本，就能获得对抗巨型机构与大规模攻击的安全杠杆。坚不可摧的个人主权身份与隐私保护，把信息与财产的防御天平第一次实质性地倾向了个体。
